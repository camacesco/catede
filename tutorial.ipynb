{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "A few examples to show how to use the classes of the package `catede` in order to estimate quantities such the Shannon entropy and the Kullback-Leibler divergence from data. \n",
    "\n",
    "## First `Experiment` class \n",
    "In these examples we generate $K=20^3$ categories distributed as sequeunces of length $L=3$ generated as a $20$ states Markov chain with random transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catede.handle_ngrams import markov_class\n",
    "from catede.estimate import Experiment\n",
    "\n",
    "seed_1 = 13273                                          # rng seed\n",
    "\n",
    "#  simulation  #\n",
    "A = 20                                                  # n. of states\n",
    "L = 3                                                   # length of the L-grams\n",
    "K = A ** L                                              # n. categories a priori\n",
    "\n",
    "mobj_1 = markov_class( L, n_states=A, seed=seed_1 )     # random Markov matrix\n",
    "\n",
    "size = int(5e4)                                         # sample size\n",
    "seqs_1 = mobj_1.generate_counts( size, seed=seed_1 )    # generate histogram of counts\n",
    "exact_sh_entropy_1 = mobj_1.exact_shannon()             # exact Shannon entropy\n",
    "\n",
    "exp_1 = Experiment( seqs_1, categories=K )              # first experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shannon entropy estimation\n",
    "\n",
    "$$ S = - \\sum_{i=1}^{K} q_{i} \\log q_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon entropy\n",
      "exact : 8.592\n",
      "naive : 8.513\n",
      "CAE : 8.574\n",
      "NSB : 8.593 +- 0.003\n"
     ]
    }
   ],
   "source": [
    "shannon_1_naive = exp_1.entropy( method='naive' ) \n",
    "shannon_1_CAE = exp_1.entropy( method='CAE' ) \n",
    "shannon_1_NSB, shannon_1_NSBstd = exp_1.entropy( method='NSB', error=True, verbose=False ) \n",
    "\n",
    "print(\"Shannon entropy\")\n",
    "print( f\"exact : { exact_sh_entropy_1:.3f}\" )\n",
    "print( f\"naive : {shannon_1_naive:.3f}\" )\n",
    "print( f\"CAE : {shannon_1_CAE:.3f}\" )\n",
    "print( f\"NSB : {shannon_1_NSB:.3f}\", r\"+-\", f\"{shannon_1_NSBstd:.3f}\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpson index estimation\n",
    "\n",
    "$$ \\lambda = \\sum_{i=1}^{K} {q_{i}}^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpson index\n",
      "exact : 0.000227\n",
      "naive : 0.000246\n",
      "CAE : 0.000238\n",
      "NSB : 0.000232 +- 0.000001\n"
     ]
    }
   ],
   "source": [
    "exact_si_idx_1 = mobj_1.exact_simpson()\n",
    "simpson_1_naive = exp_1.simpson( method='naive' )\n",
    "simpson_1_CAE = exp_1.simpson( method='CAE' ) \n",
    "simpson_1_NSB, simpson_1_NSBstd = exp_1.simpson( method='NSB', error=True, n_bins=100 )\n",
    "\n",
    "print(\"Simpson index\")\n",
    "print( f\"exact : {exact_si_idx_1:.6f}\" )\n",
    "print( f\"naive : {simpson_1_naive:.6f}\" )\n",
    "print( f\"CAE : {simpson_1_CAE:.6f}\" )\n",
    "print( f\"NSB : {simpson_1_NSB:.6f}\", r\"+-\", f\"{simpson_1_NSBstd:.6f}\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Divergence` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catede.estimate import Divergence\n",
    "\n",
    "# simulation of an independent second system\n",
    "seed_2 = 5119                                           # rng seed\n",
    "\n",
    "mobj_2 = markov_class( L, n_states=A, seed=seed_2 )     # random Markov matrix generation\n",
    "seqs_2 = mobj_2.generate_counts( size, seed=seed_2 )    # generate histogram of counts\n",
    "exact_sh_entropy_2 = mobj_2.exact_shannon()             # exact Shannon entropy  \n",
    "exp_2 = Experiment( seqs_2, categories=K )              # second experiment\n",
    "div_to1from2 = Divergence( exp_1, exp_2 )               # divergence class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler divergence estimation\n",
    "\n",
    "$$ D_{\\rm KL} = \\sum_{i=1}^{K} q_{i} \\log \\frac{q_{i}}{t_{i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kullback Leilber divergence\n",
      "exact : 0.949\n",
      "naive : 0.589\n",
      "Z : 0.826\n",
      "CMW : 0.935 +- 0.010\n"
     ]
    }
   ],
   "source": [
    "# Kullback Leibler divergence estimation #\n",
    "exact_DKL_to1from2 = mobj_1.exact_kullbackleibler( mobj_2 )\n",
    "kullback_naive = div_to1from2.kullback_leibler(method='naive')\n",
    "kullback_Z = div_to1from2.kullback_leibler( method='Zhang-Grabchak', error=True ) \n",
    "kullback_CMW, kullback_CMWstd = div_to1from2.kullback_leibler( method='CMW', error=True ) \n",
    "\n",
    "print(\"Kullback Leilber divergence\")\n",
    "print( f\"exact : { exact_DKL_to1from2:.3f}\" )\n",
    "print( f\"naive : {kullback_naive:.3f}\" )\n",
    "print( f\"Z : {kullback_Z:.3f}\" )\n",
    "print( f\"CMW : {kullback_CMW:.3f}\", r\"+-\", f\"{kullback_CMWstd:.3f}\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared Hellinger divergence estimation\n",
    "\n",
    "$$ D_{\\rm H}^2 = 1 - \\sum_{i=1}^{K} \\sqrt{q_{i}} \\sqrt{t_{i}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellinger divergence\n",
      "exact : 0.204\n",
      "naive : 0.278\n",
      "CMW : 0.201 +- 0.002\n"
     ]
    }
   ],
   "source": [
    "# Hellinger divergence estimation #\n",
    "exact_DH_to1from2 = mobj_1.exact_squared_hellinger( mobj_2 )\n",
    "hellinger_naive = div_to1from2.squared_hellinger(method='naive')\n",
    "hellinger_CMW, hellinger_CMWstd = div_to1from2.squared_hellinger( method='CMW', error=True, n_bins=1 ) \n",
    "\n",
    "print(\"Hellinger divergence\")\n",
    "print( f\"exact : { exact_DH_to1from2:.3f}\" )\n",
    "print( f\"naive : {hellinger_naive:.3f}\" )\n",
    "print( f\"CMW : {hellinger_CMW:.3f}\", r\"+-\", f\"{hellinger_CMWstd:.3f}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ca6294cbb32705ba8e9d5f1c61ac24f2b2b51f4ed14a5137959c5b73ca963d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
