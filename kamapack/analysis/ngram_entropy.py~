# coding: utf-8

'''
Francesco Camaglia, June 2020 LPENS

WARNING: 
As it is, the program works only on files with the format of generated olga files.
'''


#####################
#  LOADING MODULES  #
#####################

import os, sys
import pandas as pd
import numpy as np
import optparse
import multiprocessing

sys.path.append( os.path.realpath( __file__ ).split('kamapack')[0] )
from kamapack.handle import ngrams
from kamapack.entropy import shannon
from kamapack.utils import tryMakeDir, progress_bar, fileScope

##########
#  MAIN  #
##########

def main( 
    num, alph, max_categories, inputfile, outputfile, n_subSamples, delimiter,
    skip, beg, end, more_entr, unit_usr, saveDict
    ) :

    """
    Evaluete n-grams entropy. Example of usage:
    > python ngram_entropy.py -n 5 -i file.tsv.gz --samples 25   
    """

    #################
    #  INPUT CHECK  #
    #################
    
    # infer compression from extension
    if_scope = fileScope( inputfile )
    if delimiter is None: 
        # infer delimiter from extension
        delimiter = if_scope.delimiter
        if delimiter is None:
            raise IOError("Input file extension not recognized: please provide a delimiter.")
    else:
        try:
            delimiter = {'tab': '\t', 'space': ' ', ',': ',', ';': ';', ':': ':'}[ delimiter ]
        except KeyError:
            print("Unknown string passed as delimiter.")

    ''' 
    WARNING!: missing the rest of checks
    '''

    ########################
    #  OUTPUT DEFINITIONS  #
    ########################

    # Output folder and Entropy file name
    if outputfile == None :  
        if os.path.dirname( inputfile ) != "" :
            FLDR = os.path.dirname( inputfile )
        else :
            FLDR = "."
        outputfile = 'n' + str(num) + '-' + alph + '-s' + str(skip) + '-b' + str(beg) + 'e' + str(end)

    else :                                              
        # clean from eventual extension
        if len( outputfile.split( '.' ) ) != 1:  
            outputfile = outputfile.split('.')[0]
        # eventually create folder
        if os.path.dirname( outputfile ) != "" :
            FLDR = os.path.dirname( outputfile )
            tryMakeDir( FLDR )
            outputfile = os.path.basename( outputfile )
        else :
            FLDR = "."
            
    entropy_file = FLDR + os.sep + outputfile + '-entr.tsv'
    dict_file = FLDR + os.sep + outputfile + '-dic.tsv.gz'

    # initilialize entropy file
    SampleResults = []
    thisSampleResults = ['K_seq', 'N_ngrams', 'K_obs', 'NSB']
    if more_entr == True : 
        thisSampleResults.append( ['ML', 'MM', 'CS'] )
    SampleResults.append( thisSampleResults )

    ####################
    #  INITIALIZATION  #
    ####################
    
    # initialize dictionary of ngrams
    print( "> Creating " + str(num) + "grams dictionary file..." )
    blank_counts_list = np.zeros( len( ngrams._Alphabet_[ alph ][0]) ** num , dtype=int) 
        
    # absolute maximum number of possible categories for nrgams
    ngrams_K_max = len( blank_counts_list )                           

    df = pd.read_csv( inputfile, names=["aa","V","J","nt"], dtype=str, sep=delimiter, compression=if_scope.compression ) 
    # WARNING: check on the dataframe should be performed

    # drop duplicates and reset indexes
    df.drop_duplicates( subset= ["V","J","nt"], inplace=True )
    df.reset_index(drop=True, inplace=True)
    
    # definition of maximmum number of categories
    if max_categories == None :
        max_categories = len(df)

    #  subsamples definition
    if n_subSamples == 1 :
        # option for 1 single measure  
        K_vec = [ max_categories ]
    else :
        # logarithmic subsample sets 
        K_vec = list( np.logspace( np.log10( 5e2 ), np.log10( max_categories ), num = n_subSamples ).astype(int) )
    
    # check for max_categories greater than dataframe and change subsamples
    if max_categories > len( df ) :
        K_vec = [ i for i in K_vec if i < len(df) ]
        K_vec.append( len(df) )

    #################                                                                                                                    
    #  USER SUMMAR  #                                                                                                                     
    #################                                                                                                                    

    print("> Computation of "+str(num)+"grams entropy of sequences with alphabet: " + ngrams._Alphabet_[alph][1] )
    print("> Maximum number of clonotypes considered: ", max_categories)
    print("> Number of subsamples considered: ", n_subSamples)
    print("> skip: ", skip, "| beginning: ", beg, "| end: ", end)
    print("> Input file: ", inputfile )
    print("> Output file: ", entropy_file )
    print("> Max number of cores detected: ", multiprocessing.cpu_count() )

    ###############
    #  EXECUTION  #
    ###############

    progress_bar( 0., status=1 )

    for subsamp_indx in range( 0, len(K_vec), 1 ) :

        n_categ = K_vec[ subsamp_indx ]
        df_subsamp = df.sample( n = n_categ )    
                                         
        # the subsample counts_list and estimators are computed
        counts_list = ngrams.counts_list_update(
            np.zeros( len( ngrams._Alphabet_[ alph ][0]) ** num , dtype=int ) , 
            df_subsamp["aa"].values, 
            num=num, alph=alph, 
            skip=skip, beg=beg, end=end
            ) 

        exp = shannon.experiment( counts_list , categories = ngrams_K_max )

        thisSampleResults = [ n_categ, exp.N, exp.obs_categ ]
        thisSampleResults.append( exp.entropy( method="NSB", unit = unit_usr ) )
        
        if more_entr == True :
            thisSampleResults.append( exp.entropy( method="ML", unit = unit_usr ) )
            thisSampleResults.append( exp.entropy( method="MM", unit = unit_usr ) )
            thisSampleResults.append( exp.entropy( method="CS", unit = unit_usr ) )

        SampleResults.append( thisSampleResults )   

        progress_bar( ( subsamp_indx +  1 ) / len(K_vec) , status=1 )   
  
    # >>>>>>>>>>>>>>
    #   CONCLUSION
    # >>>>>>>>>>>>>>

    # save entropy of subsamples
    pd.DataFrame( SampleResults ).to_csv( entropy_file, sep="\t", quoting=None, index=None, header=False )

    # save final dictionary of ngrams count if requested
    ngrams.save_file_dict( num, alph, counts_list, dict_file )
    
    progress_bar( 1., status=2 )
###



################
#  EXECUTABLE  #
################

if __name__ == "__main__" :

    parser = optparse.OptionParser( conflict_handler="resolve" )

    #####################
    # MANDATORY OPTIONS #
    #####################
    
    parser.add_option( '-n', action='store', dest='num', type="int", help='The "n-grams" length.' )
    parser.add_option( '-a', '--alphabet', action='store', dest='alph', type="string", default="AA", help="The alphabet of the reads." )
    parser.add_option( '-M', '--max_categories', action='store', dest='max_categories', type="int", help="The maximum number of sequences considered." )
    parser.add_option( '-i', '--inputfile', action="store", dest = 'inputfile', metavar='PATH/TO/FILE', type="string", help='PATH/TO/FILE where Olga sequences are stored.' )

    ######################
    #  SPECIFIC OPTIONS  #
    ######################

    parser.add_option( '-s', '--skip', action="store", dest="skip", type="int", default=0, help="" )
    parser.add_option( '-B', '--beginning_const', action="store", dest="beg", type="int", default=0, help="" )
    parser.add_option( '-E', '--end_const', action="store", dest="end", type="int", default=0, help="" )
    parser.add_option( '-u','--unit', action='store', dest='unit_usr', type='choice', choices = [ "ln", "log2", "log10" ], default="log2", help="The unit of the logarithm." )

    ##################
    #  USER OPTIONS  #
    ##################

    parser.add_option( '-d', '--delimiter', action='store', dest='delimiter', type='choice', choices=['tab', 'space', ',', ';', ':'], default=None, help="Inputfile delimiter." )
    parser.add_option( '--samples', action='store', dest='n_subSamples', type="int", default=1, help="Number of subsamples (including all dataset)." )
    parser.add_option( '-o', '--outputfile', action="store", dest='outputfile', metavar='PATH/TO/FILE', type="string", default=None, help='The file where entropy has to be saved.' )
    parser.add_option( '--more_entropies', action='store_true', dest='more_entr', default=False, help="Wheter to compute other entropy estimators alongside with NSB." )
    parser.add_option( '--save_dictionary', action='store_true', dest='saveDict', default=False, help="Wheter to save the dictionary with counts at the end." )

    ######################
    # OPTIONS ASSIGNMENT #
    ######################

    options, args = parser.parse_args()

    main(
        num = options.num,
        alph = options.alph,
        max_categories = options.max_categories,
        inputfile = options.inputfile,
        outputfile = options.outputfile,
        n_subSamples = options.n_subSamples,
        delimiter = options.delimiter,
        skip = options.skip,
        beg = options.beg,
        end = options.end,
        more_entr = options.more_entr,
        unit_usr = options.unit_usr,
        saveDict = options.saveDict
    )
